{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dc2b3b",
   "metadata": {},
   "source": [
    "# 2950 Project Phase 2\n",
    "\n",
    "Flavia Jiang (yj472), Rachel Wang (jw879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8f4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b42ba",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd70b62",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354d88e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "1.What are the observations (rows) and the attributes (columns)?\n",
    "\n",
    "\n",
    "2. Why was this dataset created?\n",
    "This dataset was created to investigate and analyze various aspects of human dating behavior in the context of speed dating. \n",
    "Researchers were interested in understanding how individuals(males and females) make dating decisions, what attributes they \n",
    "consider important, and how different factors influence the outcomes of speed dating encounters.\n",
    "\n",
    "3. Who funded the creation of the dataset?\n",
    "\n",
    "The dataset was collected as part of academic research. \n",
    "\n",
    "4. What processes might have influenced what data was observed and recorded and what was not?\n",
    "\n",
    "Participant Demographics: The age, gender, and demographic characteristics of the participants could influence the data collected. \n",
    "In this study, all subjects are from graduate and professional school of Columbia University.\n",
    "\n",
    "Self-Selection of Participants: Participants in the speed dating experiment were volunteers, which means they self-selected to take part. \n",
    "This self-selection process may have introduced biases, as those who chose to participate might have different preferences or characteristics \n",
    "compared to the general population. This could impact the generalizability of the findings.\n",
    "\n",
    "Experiment Design: The design of the speed dating experiment determined what data could be collected. The researchers structured the experiment, \n",
    "including the number of participants, the number of potential partners, and the available information about each partner. The experimental \n",
    "conditions may not fully represent real-world dating situations.\n",
    "\n",
    "Survey Responses: The data collected was based on surveys and questionnaires filled out by participants. Data collection relied on participants' \n",
    "willingness to respond honestly and accurately, which could be influenced by social desirability bias or other factors.\n",
    "\n",
    "5. What preprocessing was done, and how did the data come to be in the form that you are using?\n",
    "\n",
    "\n",
    "6. If people are involved, were they aware of the data collection, and if so, what purpose did they expect the data to be used for?\n",
    "\n",
    "Participants in the speed dating events would have been aware of the data collection process, as informed consent is a standard practice \n",
    "in research involving human subjects. They would have been informed about the purpose of the data collection, which is typically for academic \n",
    "research. Participants would have expected the data to be used to study dating behavior and potentially contribute to our understanding of human \n",
    "interactions and preferences.\n",
    "\n",
    "7. Where can your raw source data be found, if applicable? Provide a link to the raw data\n",
    "\n",
    "Link to the dataset and the documentation: http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8636c1c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa6b13",
   "metadata": {},
   "source": [
    "Firstly, load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c0adc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8378, 195)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df = pd.read_csv(\"speed_dating_data.csv\", encoding=\"ISO-8859-1\")\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cab65e",
   "metadata": {},
   "source": [
    "### Remove unnecessary columns\n",
    "The data set is super wide, and there are so many variables. So we went through the variable codebook made by the creators of this data set and removed variables we currently think would be definitely unnecessary for our future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563653ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8378, 163)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_list = ['idg', \n",
    "               'condtn', \n",
    "               'position', \n",
    "               'positin1', \n",
    "               'int_corr', \n",
    "               'undergra', \n",
    "               'mn_sat', \n",
    "               'zipcode', \n",
    "               'tuition', \n",
    "               'length', \n",
    "               'you_call', \n",
    "               'them_cal', \n",
    "               'numdat_2', \n",
    "               'num_in_3', \n",
    "               'go_out',\n",
    "               'id', \n",
    "               'partner', \n",
    "               'int_corr', \n",
    "               'pf_o_att', \n",
    "               'pf_o_sin', \n",
    "               'pf_o_int', \n",
    "               'pf_o_fun', \n",
    "               'pf_o_amb', \n",
    "               'pf_o_sha', \n",
    "               'from', \n",
    "               'income', \n",
    "               'goal', \n",
    "               'date', \n",
    "               'exphappy', \n",
    "               'satis_2', \n",
    "               'date_3', \n",
    "               'numdat_3',\n",
    "               'expnum']\n",
    "    \n",
    "if delete_list[0] in dating_df:\n",
    "    dating_df = dating_df.drop(delete_list, axis = 1)\n",
    "    \n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfdfc63",
   "metadata": {},
   "source": [
    "### Rename columns\n",
    "Now, we still have many columns. Certainly, we won't use all of them, but we kept them there because they are potential predictors that might help train the model. And we don't want to restrict ourselves to a specific set of variables. \n",
    "\n",
    "Though we did not do any web scraping or merge data sets to get this data frame, one main effort we made was to interpret the meaning of each variable -- carefully reading through the 15-paged variable codebook. We noticed that some of the names of the given variable names were vague. So, we decided to change them so they conveyed more straightforward information about the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5736c77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>wave</th>\n",
       "      <th>num_dates</th>\n",
       "      <th>order</th>\n",
       "      <th>partner_id</th>\n",
       "      <th>match</th>\n",
       "      <th>samerace</th>\n",
       "      <th>partner_age</th>\n",
       "      <th>partner_race</th>\n",
       "      <th>...</th>\n",
       "      <th>attr3_3</th>\n",
       "      <th>sinc3_3</th>\n",
       "      <th>intel3_3</th>\n",
       "      <th>fun3_3</th>\n",
       "      <th>amb3_3</th>\n",
       "      <th>attr5_3</th>\n",
       "      <th>sinc5_3</th>\n",
       "      <th>intel5_3</th>\n",
       "      <th>fun5_3</th>\n",
       "      <th>amb5_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  gender  wave  num_dates  order  partner_id  match  samerace  \\\n",
       "0   1       0     1         10      4        11.0      0         0   \n",
       "1   1       0     1         10      3        12.0      0         0   \n",
       "2   1       0     1         10     10        13.0      1         1   \n",
       "3   1       0     1         10      5        14.0      1         0   \n",
       "4   1       0     1         10      7        15.0      1         0   \n",
       "\n",
       "   partner_age  partner_race  ...  attr3_3  sinc3_3  intel3_3  fun3_3  amb3_3  \\\n",
       "0         27.0           2.0  ...      5.0      7.0       7.0     7.0     7.0   \n",
       "1         22.0           2.0  ...      5.0      7.0       7.0     7.0     7.0   \n",
       "2         22.0           4.0  ...      5.0      7.0       7.0     7.0     7.0   \n",
       "3         23.0           2.0  ...      5.0      7.0       7.0     7.0     7.0   \n",
       "4         24.0           3.0  ...      5.0      7.0       7.0     7.0     7.0   \n",
       "\n",
       "   attr5_3  sinc5_3  intel5_3  fun5_3  amb5_3  \n",
       "0      NaN      NaN       NaN     NaN     NaN  \n",
       "1      NaN      NaN       NaN     NaN     NaN  \n",
       "2      NaN      NaN       NaN     NaN     NaN  \n",
       "3      NaN      NaN       NaN     NaN     NaN  \n",
       "4      NaN      NaN       NaN     NaN     NaN  \n",
       "\n",
       "[5 rows x 163 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df = dating_df.rename(columns = {\"iid\":\"id\",\n",
    "                          \"age_o\":\"partner_age\",\n",
    "                          \"race_o\":\"partner_race\", \n",
    "                          \"round\": \"num_dates\", \n",
    "                          \"pid\": \"partner_id\", \n",
    "                          \"dec_o\": \"partner_decision\", \n",
    "                          \"match_es\": \"num_matches_estimated\"})\n",
    "dating_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb599d3",
   "metadata": {},
   "source": [
    "### Remove biased data points\n",
    "As described in Data Description part, the researchers ran 21 speed dating sessions, or waves, in total. However, as they explained in their paper, they removed four sessions (waves 18-21) from the analysis \"because they involved an experimental intervention where participants were asked to bring their favorite book. These four sessions were run specifically to study how decision weights and selectivity would be affected by an intervention designed to shift subjects’ attention away from superficial physical attributes. The inclusion of these four sessions does not alter the results reported below; they are omitted so that the only experimental difference across sessions is group size.\" Accordingly we also removed data for these four sessions. \n",
    "\n",
    "The researchers also said they removed another wave (#12) because they \"imposed a maximum number of acceptances\" on participants of this wave. We thought this restriction would affect participants' decisions, so we also removed this wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7076d1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6412, 163)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F: filter (remove some waves, deal with NAs)\n",
    "dating_df = dating_df[~dating_df['wave'].isin([12, 18, 19, 20, 21])]\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a07474a",
   "metadata": {},
   "source": [
    "The researchers also mentioned they removed two more sessions from their analysis because they thought their participants were too few. We thought this was not a big concern as of now. So we decided to keep them.\n",
    "\n",
    "We had another concern, which the researchers did not address in their analysis. We noitced that for many of the survey questions, the researchers let participants of waves 6-9 rate the importance of six attributes separately on a scale of 1-10 and let participants of other waves distribute 100 points to these six attributes. Let's investigate this in the dataset with an example involving variables attr1_1, sinc1_1, intel1_1, fun1_1, amb1_1, shar1_1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b737a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attr1_1</th>\n",
       "      <th>sinc1_1</th>\n",
       "      <th>intel1_1</th>\n",
       "      <th>fun1_1</th>\n",
       "      <th>amb1_1</th>\n",
       "      <th>shar1_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      attr1_1  sinc1_1  intel1_1  fun1_1  amb1_1  shar1_1\n",
       "1846    16.67    16.67     16.67   16.67   16.67    16.67\n",
       "1847    16.67    16.67     16.67   16.67   16.67    16.67\n",
       "1848    16.67    16.67     16.67   16.67   16.67    16.67\n",
       "1849    16.67    16.67     16.67   16.67   16.67    16.67\n",
       "1850    16.67    16.67     16.67   16.67   16.67    16.67"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = dating_df[(dating_df['wave'] > 5) & (dating_df['wave'] < 10)]\n",
    "temp[['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fde2e0",
   "metadata": {},
   "source": [
    "As we could infer from the values, the researchers converted the data for waves 6-9 so that for each set of variables for six attributes, they added up to 100 points. For example, if a participant of wave 6 rated these six attributes all as very important (that is, answered 10s to these six questions), it would be converted to (16.67, 16.67, 16.67, 16.67, 16.67, 16.67) so they summed to 100.\n",
    "\n",
    "They did this so that they could still include waves 6-9 in the analysis. But this rude conversion could be problemetic. The simple logic is that the participant in the above example might think all six attributes were very important but not equally important. Then the adjusted data was not an accurate representation of the participant's beliefs.\n",
    "\n",
    "Note that there were many sets of six variables like this because the researchers asked the participants to rate the importance of six attributes in different settings. And we were quite sure we would use at least one set of variable like this. So we decided to remove waves 6-9 from our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87ee2ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4850, 163)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df = dating_df[~dating_df['wave'].isin([6, 7, 8, 9])]\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec43bf",
   "metadata": {},
   "source": [
    "We know so far we had removed about half of the rows from the dataframe. But we still have 4000+ data points, which from a statistical standpoint are enough for regression and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2510a11",
   "metadata": {},
   "source": [
    "### Remove inaccurate data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5698d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: verify 100 points (1_1, 4_1, 2_1, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb0e5e",
   "metadata": {},
   "source": [
    "### Map coded categorical variables to their corresponding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36643b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## F: categorical variable mapping: field, race, career"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ecfc88",
   "metadata": {},
   "source": [
    "### Deal with missing values\n",
    "We noticed for some variables there were many missing values due to how the experiment was designed and conducted. For each variable with missing values, we re-examined whether we still thought it would be a potential good predictor in our future modeling given the fact that including it would make the model less robust. We made decisions between keeping or removing the columns accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "602bb8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['partner_id', 10]\n",
      "['partner_age', 77]\n",
      "['partner_race', 68]\n",
      "['attr_o', 132]\n",
      "['sinc_o', 182]\n",
      "['intel_o', 188]\n",
      "['fun_o', 215]\n",
      "['amb_o', 435]\n",
      "['shar_o', 661]\n",
      "['like_o', 136]\n",
      "['prob_o', 176]\n",
      "['met_o', 231]\n",
      "['age', 68]\n",
      "['field', 58]\n",
      "['field_cd', 77]\n",
      "['race', 58]\n",
      "['imprace', 74]\n",
      "['imprelig', 74]\n",
      "['career', 84]\n",
      "['career_c', 133]\n",
      "['sports', 74]\n",
      "['tvsports', 74]\n",
      "['exercise', 74]\n",
      "['dining', 74]\n",
      "['museums', 74]\n",
      "['art', 74]\n",
      "['hiking', 74]\n",
      "['gaming', 74]\n",
      "['clubbing', 74]\n",
      "['reading', 74]\n",
      "['tv', 74]\n",
      "['theater', 74]\n",
      "['movies', 74]\n",
      "['concerts', 74]\n",
      "['music', 74]\n",
      "['shopping', 74]\n",
      "['yoga', 74]\n",
      "['attr1_1', 74]\n",
      "['sinc1_1', 74]\n",
      "['intel1_1', 74]\n",
      "['fun1_1', 84]\n",
      "['amb1_1', 94]\n",
      "['shar1_1', 94]\n",
      "['attr4_1', 1884]\n",
      "['sinc4_1', 1884]\n",
      "['intel4_1', 1884]\n",
      "['fun4_1', 1884]\n",
      "['amb4_1', 1884]\n",
      "['shar4_1', 1884]\n",
      "['attr2_1', 74]\n",
      "['sinc2_1', 74]\n",
      "['intel2_1', 74]\n",
      "['fun2_1', 74]\n",
      "['amb2_1', 84]\n",
      "['shar2_1', 84]\n",
      "['attr3_1', 100]\n",
      "['sinc3_1', 100]\n",
      "['fun3_1', 100]\n",
      "['intel3_1', 100]\n",
      "['amb3_1', 100]\n",
      "['attr5_1', 1910]\n",
      "['sinc5_1', 1910]\n",
      "['intel5_1', 1910]\n",
      "['fun5_1', 1910]\n",
      "['amb5_1', 1910]\n",
      "['attr', 122]\n",
      "['sinc', 172]\n",
      "['intel', 178]\n",
      "['fun', 205]\n",
      "['amb', 425]\n",
      "['shar', 652]\n",
      "['like', 126]\n",
      "['prob', 167]\n",
      "['met', 221]\n",
      "['num_matches_estimated', 390]\n",
      "['attr1_s', 2785]\n",
      "['sinc1_s', 2785]\n",
      "['intel1_s', 2785]\n",
      "['fun1_s', 2785]\n",
      "['amb1_s', 2785]\n",
      "['shar1_s', 2785]\n",
      "['attr3_s', 2806]\n",
      "['sinc3_s', 2806]\n",
      "['intel3_s', 2806]\n",
      "['fun3_s', 2806]\n",
      "['amb3_s', 2806]\n",
      "['attr7_2', 4269]\n",
      "['sinc7_2', 4298]\n",
      "['intel7_2', 4269]\n",
      "['fun7_2', 4269]\n",
      "['amb7_2', 4298]\n",
      "['shar7_2', 4279]\n",
      "['attr1_2', 542]\n",
      "['sinc1_2', 524]\n",
      "['intel1_2', 524]\n",
      "['fun1_2', 524]\n",
      "['amb1_2', 524]\n",
      "['shar1_2', 524]\n",
      "['attr4_2', 2212]\n",
      "['sinc4_2', 2212]\n",
      "['intel4_2', 2212]\n",
      "['fun4_2', 2212]\n",
      "['amb4_2', 2212]\n",
      "['shar4_2', 2212]\n",
      "['attr2_2', 2212]\n",
      "['sinc2_2', 2212]\n",
      "['intel2_2', 2212]\n",
      "['fun2_2', 2212]\n",
      "['amb2_2', 2212]\n",
      "['shar2_2', 2212]\n",
      "['attr3_2', 524]\n",
      "['sinc3_2', 524]\n",
      "['intel3_2', 524]\n",
      "['fun3_2', 524]\n",
      "['amb3_2', 524]\n",
      "['attr5_2', 2212]\n",
      "['sinc5_2', 2212]\n",
      "['intel5_2', 2212]\n",
      "['fun5_2', 2212]\n",
      "['amb5_2', 2212]\n",
      "['attr1_3', 2594]\n",
      "['sinc1_3', 2594]\n",
      "['intel1_3', 2594]\n",
      "['fun1_3', 2594]\n",
      "['amb1_3', 2594]\n",
      "['shar1_3', 2594]\n",
      "['attr7_3', 3609]\n",
      "['sinc7_3', 3609]\n",
      "['intel7_3', 3609]\n",
      "['fun7_3', 3609]\n",
      "['amb7_3', 3609]\n",
      "['shar7_3', 3609]\n",
      "['attr4_3', 3609]\n",
      "['sinc4_3', 3609]\n",
      "['intel4_3', 3609]\n",
      "['fun4_3', 3609]\n",
      "['amb4_3', 3609]\n",
      "['shar4_3', 3609]\n",
      "['attr2_3', 3609]\n",
      "['sinc2_3', 3609]\n",
      "['intel2_3', 3609]\n",
      "['fun2_3', 3609]\n",
      "['amb2_3', 3609]\n",
      "['shar2_3', 3609]\n",
      "['attr3_3', 2594]\n",
      "['sinc3_3', 2594]\n",
      "['intel3_3', 2594]\n",
      "['fun3_3', 2594]\n",
      "['amb3_3', 2594]\n",
      "['attr5_3', 3609]\n",
      "['sinc5_3', 3609]\n",
      "['intel5_3', 3609]\n",
      "['fun5_3', 3609]\n",
      "['amb5_3', 3609]\n"
     ]
    }
   ],
   "source": [
    "for col in dating_df:\n",
    "    n = sum(pd.isna(dating_df[col]))\n",
    "    if (n > 0):\n",
    "        print([col, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfcd660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4850, 84)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_list = []\n",
    "for i in ['4_1', '4_2', '4_3', '5_1', '1_s', '3_s', '7_2', '2_2', '5_2', '1_3', '7_3', '2_3', '3_3', '5_3']:\n",
    "    new = [var + i for var in ['attr', 'sinc', 'intel', 'fun', 'amb', 'shar']]\n",
    "    delete_list = delete_list + new\n",
    "\n",
    "for var in ['shar5_1', 'shar3_s', 'shar5_2', 'shar3_3', 'shar5_3']:\n",
    "    delete_list.remove(var)\n",
    "\n",
    "if delete_list[0] in dating_df:\n",
    "    dating_df = dating_df.drop(delete_list, axis = 1)\n",
    "    \n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e4deb",
   "metadata": {},
   "source": [
    "Next, let's see how many rows would be left if all rows with any missing values were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3469273e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2578, 84)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df.dropna().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff1501",
   "metadata": {},
   "source": [
    "A total of 2578 datapoints did not sound really bad, but we did not choose to do this because we were still deciding on the set of predictors. We should leave them there until we have final set of predictors based on the exploratory analysis below and the initial modeling phase. But from now on, we should be really carefully with NA values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deba2a",
   "metadata": {},
   "source": [
    "### Convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8449f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: correct datatype (e.g. partner_id ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677617b",
   "metadata": {},
   "source": [
    "## Data Limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49819de6",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c11f3cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4850, 84)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R: general: dimension\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0260d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: one-variable: mean, sd, #observations\n",
    "# for numerical variable: dating_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ed91864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: one-variable: plot distribution\n",
    "# histogram for numeric variable (reference: kaggle example)\n",
    "# pie chart for categorical variable (reference: discussion example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24551206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F: two variable\n",
    "# change over Time1, during event, Time 2, Time 3\n",
    "# correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

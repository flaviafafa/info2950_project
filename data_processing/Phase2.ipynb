{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dc2b3b",
   "metadata": {},
   "source": [
    "# 2950 Project Phase 2\n",
    "\n",
    "Flavia Jiang (yj472), Rachel Wang (jw879)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "df8f4913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b42ba",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ccccd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fd70b62",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecd165",
   "metadata": {},
   "source": [
    "1.What are the observations (rows) and the attributes (columns)?\n",
    "\n",
    "\n",
    "2. Why was this dataset created?\n",
    "This dataset was created to investigate and analyze various aspects of human dating behavior in the context of speed dating. \n",
    "Researchers were interested in understanding how individuals(males and females) make dating decisions, what attributes they \n",
    "consider important, and how different factors influence the outcomes of speed dating encounters.\n",
    "\n",
    "3. Who funded the creation of the dataset?\n",
    "\n",
    "The dataset was collected as part of academic research. \n",
    "\n",
    "4. What processes might have influenced what data was observed and recorded and what was not?\n",
    "\n",
    "Participant Demographics: The age, gender, and demographic characteristics of the participants could influence the data collected. \n",
    "In this study, all subjects are from graduate and professional school of Columbia University.\n",
    "\n",
    "Self-Selection of Participants: Participants in the speed dating experiment were volunteers, which means they self-selected to take part. \n",
    "This self-selection process may have introduced biases, as those who chose to participate might have different preferences or characteristics \n",
    "compared to the general population. This could impact the generalizability of the findings.\n",
    "\n",
    "Experiment Design: The design of the speed dating experiment determined what data could be collected. The researchers structured the experiment, \n",
    "including the number of participants, the number of potential partners, and the available information about each partner. The experimental \n",
    "conditions may not fully represent real-world dating situations.\n",
    "\n",
    "Survey Responses: The data collected was based on surveys and questionnaires filled out by participants. Data collection relied on participants' \n",
    "willingness to respond honestly and accurately, which could be influenced by social desirability bias or other factors.\n",
    "\n",
    "5. What preprocessing was done, and how did the data come to be in the form that you are using?\n",
    "\n",
    "\n",
    "6. If people are involved, were they aware of the data collection, and if so, what purpose did they expect the data to be used for?\n",
    "\n",
    "Participants in the speed dating events would have been aware of the data collection process, as informed consent is a standard practice \n",
    "in research involving human subjects. They would have been informed about the purpose of the data collection, which is typically for academic \n",
    "research. Participants would have expected the data to be used to study dating behavior and potentially contribute to our understanding of human \n",
    "interactions and preferences.\n",
    "\n",
    "7. Where can your raw source data be found, if applicable? Provide a link to the raw data\n",
    "\n",
    "Link to the dataset and the documentation: http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8636c1c",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaa6b13",
   "metadata": {},
   "source": [
    "Firstly, load the data. The data set is wide and long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "4c0adc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8378, 195)"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df = pd.read_csv(\"speed_dating_data.csv\", encoding=\"ISO-8859-1\")\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cab65e",
   "metadata": {},
   "source": [
    "### Select necessary columns\n",
    "The data set is super wide, and there are so many variables. So we went through the codebook made by the creators of this data set and selected variables we currently think would be necessary for our future analysis. Now we still have 41 columns. Certainly, we won't use all of them in the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "563653ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8378, 41)"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_list = ['iid', 'gender', 'wave', 'round', 'pid', 'samerace', 'age_o', 'age', 'field_cd', \n",
    "               'race','career_c', 'sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', \n",
    "               'hiking', 'gaming', 'clubbing', 'reading', 'tv','theater', 'movies', 'concerts', \n",
    "               'music', 'shopping', 'yoga', 'attr3_1','sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', \n",
    "               'dec', 'attr', 'sinc','intel', 'fun', 'amb', 'shar', 'prob']\n",
    "\n",
    "dating_df = dating_df[select_list]\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfdfc63",
   "metadata": {},
   "source": [
    "### Rename columns\n",
    "\n",
    "Though we did not do any web scraping or merge data sets to get this data frame, one main effort we made was to interpret the meaning of each variable -- carefully reading through the 15-paged codebook. We noticed that some of the names of the given variable names were vague. So, we decided to change them so they conveyed more straightforward information about the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "5736c77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>gender</th>\n",
       "      <th>wave</th>\n",
       "      <th>num_dates</th>\n",
       "      <th>partner_id</th>\n",
       "      <th>same_race</th>\n",
       "      <th>partner_age</th>\n",
       "      <th>age</th>\n",
       "      <th>field_cd</th>\n",
       "      <th>race</th>\n",
       "      <th>...</th>\n",
       "      <th>intel3_1</th>\n",
       "      <th>amb3_1</th>\n",
       "      <th>decision</th>\n",
       "      <th>attr</th>\n",
       "      <th>sinc</th>\n",
       "      <th>intel</th>\n",
       "      <th>fun</th>\n",
       "      <th>amb</th>\n",
       "      <th>shar</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  gender  wave  num_dates  partner_id  same_race  partner_age   age  \\\n",
       "0   1       0     1         10        11.0          0         27.0  21.0   \n",
       "1   1       0     1         10        12.0          0         22.0  21.0   \n",
       "2   1       0     1         10        13.0          1         22.0  21.0   \n",
       "3   1       0     1         10        14.0          0         23.0  21.0   \n",
       "4   1       0     1         10        15.0          0         24.0  21.0   \n",
       "\n",
       "   field_cd  race  ...  intel3_1  amb3_1  decision  attr  sinc  intel  fun  \\\n",
       "0       1.0   4.0  ...       8.0     7.0         1   6.0   9.0    7.0  7.0   \n",
       "1       1.0   4.0  ...       8.0     7.0         1   7.0   8.0    7.0  8.0   \n",
       "2       1.0   4.0  ...       8.0     7.0         1   5.0   8.0    9.0  8.0   \n",
       "3       1.0   4.0  ...       8.0     7.0         1   7.0   6.0    8.0  7.0   \n",
       "4       1.0   4.0  ...       8.0     7.0         1   5.0   6.0    7.0  7.0   \n",
       "\n",
       "   amb  shar  prob  \n",
       "0  6.0   5.0   6.0  \n",
       "1  5.0   6.0   5.0  \n",
       "2  5.0   7.0   NaN  \n",
       "3  6.0   8.0   6.0  \n",
       "4  6.0   6.0   6.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df = dating_df.rename(columns = {\"iid\":\"id\",\n",
    "                                        \"age_o\":\"partner_age\",\n",
    "                                        \"round\": \"num_dates\", \n",
    "                                        \"pid\": \"partner_id\", \n",
    "                                        \"samerace\": \"same_race\", \n",
    "                                        \"dec\": \"decision\"})\n",
    "dating_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb599d3",
   "metadata": {},
   "source": [
    "### Remove biased data points\n",
    "As described in Data Description part, the researchers ran 21 speed dating sessions, or waves, in total. However, as they explained in their paper, they removed four sessions (waves 18-21) from the analysis \"because they involved an experimental intervention where participants were asked to bring their favorite book. These four sessions were run specifically to study how decision weights and selectivity would be affected by an intervention designed to shift subjects’ attention away from superficial physical attributes. The inclusion of these four sessions does not alter the results reported below; they are omitted so that the only experimental difference across sessions is group size.\" Accordingly we also removed data for these four sessions. \n",
    "\n",
    "The researchers also said they removed another wave (#12) because they \"imposed a maximum number of acceptances\" on participants of this wave. We thought this restriction would affect participants' decisions, so we also removed this wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "id": "7076d1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6412, 41)"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df = dating_df[~dating_df['wave'].isin([12, 18, 19, 20, 21])]\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ecfc88",
   "metadata": {},
   "source": [
    "### Deal with missing values\n",
    "We noticed there were many missing values due to how the experiment was designed and conducted. For each variable with more than 200 missing values, we re-examined whether we still thought it would be a potential good predictor in our future modeling given the fact that including it would make the model less robust. Finally we decided to remove the variable called \"shar,\" which was the dater's rating of shared interests/hobbies for the datee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "602bb8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['partner_id', 10]\n",
      "['partner_age', 82]\n",
      "['age', 73]\n",
      "['field_cd', 82]\n",
      "['race', 63]\n",
      "['career_c', 138]\n",
      "['sports', 79]\n",
      "['tvsports', 79]\n",
      "['exercise', 79]\n",
      "['dining', 79]\n",
      "['museums', 79]\n",
      "['art', 79]\n",
      "['hiking', 79]\n",
      "['gaming', 79]\n",
      "['clubbing', 79]\n",
      "['reading', 79]\n",
      "['tv', 79]\n",
      "['theater', 79]\n",
      "['movies', 79]\n",
      "['concerts', 79]\n",
      "['music', 79]\n",
      "['shopping', 79]\n",
      "['yoga', 79]\n",
      "['attr3_1', 105]\n",
      "['sinc3_1', 105]\n",
      "['fun3_1', 105]\n",
      "['intel3_1', 105]\n",
      "['amb3_1', 105]\n",
      "['attr', 130]\n",
      "['sinc', 196]\n",
      "['intel', 208]\n",
      "['fun', 260]\n",
      "['amb', 553]\n",
      "['shar', 874]\n",
      "['prob', 206]\n"
     ]
    }
   ],
   "source": [
    "for col in dating_df:\n",
    "    n = sum(pd.isna(dating_df[col]))\n",
    "    if (n > 0):\n",
    "        print([col, n])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114e4deb",
   "metadata": {},
   "source": [
    "Next, let's see how many rows would be left if all rows with any missing values were dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "3469273e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5493, 40)"
      ]
     },
     "execution_count": 828,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dating_df = dating_df.drop([\"shar\"], axis = 1)\n",
    "dating_df = dating_df.dropna()\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71c4bd",
   "metadata": {},
   "source": [
    "From a statistical standpoint, 5079 datapoints were good enough for a robust logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cb0e5e",
   "metadata": {},
   "source": [
    "### Map coded categorical variables to their corresponding values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d50e4",
   "metadata": {},
   "source": [
    "We also noticed that some categorical variables were coded as integers (e.g., field, race, career). This was for data storage and system performance reasons. For our purposes, we thought it would be better if these variables were presented as the actual values rather than the integer codes so that we could visualize and analyze them more efficiently. So we did the following conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "id": "36643b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dating_df[\"field\"] = dating_df[\"field_cd\"].map({1:\"Law\", 2:\"Math\", 3:\"Social Science, Psychologist\", \n",
    "                                                4:\"Medical Science, Pharmaceuticals, and Bio Tech\", \n",
    "                                                5:\"Engineering\", 6:\"English/Creative Writing/Journalism\", \n",
    "                                                7:\"History/Religion/Philosophy\", 8:\"Business/Econ/Finance\", \n",
    "                                                9:\"Education, Academia\", 10:\"Biological Sciences/Chemistry/Physics\", \n",
    "                                                11:\"Social Work\", 12:\"Undergrad/undecided\", \n",
    "                                                13:\"Political Science/International Affairs\", 14:\"Film\", \n",
    "                                                15:\"Fine Arts/Arts Administration\", 16:\"Languages\", \n",
    "                                                17:\"Architecture\", 18:\"Other\"})\n",
    "dating_df = dating_df.drop([\"field_cd\"], axis=1)\n",
    "\n",
    "dating_df[\"race\"] = dating_df[\"race\"].map({1: \"Black/African American\", 2:\"European/Caucasian-American\", \n",
    "                                           3:\"Latino/Hispanic American\", 4:\"Asian/Pacific Islander/Asian-American\", \n",
    "                                           5:\"Native American\", 6:\"Other\"})\n",
    "\n",
    "dating_df[\"career\"] = dating_df[\"career_c\"].map({1:\"Lawyer \", 2:\"Academic/Research\", 3:\"Psychologist\",4:\"Doctor/Medicine\",\n",
    "                                                  5:\"Engineer\", 6:\"Creative Arts/Entertainment\", \n",
    "                                                  7:\"Banking/Consulting/Finance/Marketing/Business/CEO/Entrepreneur/Admin\", \n",
    "                                                  8:\"Real Estate\", 9:\"International/Humanitarian Affairs\", 10:\"Undecided\", \n",
    "                                                  11:\"Social Work\", 12:\"Speech Pathology\", 13:\"Politics\", 14:\"Pro sports/Athletics\", \n",
    "                                                  15:\"Other\", 16:\"Journalism\", 17:\"Architecture\"})\n",
    "dating_df = dating_df.drop([\"career_c\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deba2a",
   "metadata": {},
   "source": [
    "### Convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "8449f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: correct datatype (e.g. partner_id ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2510a11",
   "metadata": {},
   "source": [
    "### Remove inaccurate data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "a5698d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: verify 100 points ('attr1_1','sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1', 'attr3_1', 'sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1', 'attr', 'sinc','intel', 'fun', 'amb', 'shar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab055180",
   "metadata": {},
   "source": [
    "### Preprocess regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "608c5f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F: make new variables: age difference, interest difference, and others\n",
    "\n",
    "# age_diff\n",
    "similarity_df = dating_df\n",
    "similarity_df[\"age_diff\"] = similarity_df[\"partner_age\"] - similarity_df[\"age\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "6c0196d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  partner_id  sports  tvsports  exercise  dining  museums  art  hiking  \\\n",
      "0   4        11.0     1.0       1.0       6.0     7.0      6.0  7.0     7.0   \n",
      "1   4        12.0     1.0       1.0       6.0     7.0      6.0  7.0     7.0   \n",
      "2   4        13.0     1.0       1.0       6.0     7.0      6.0  7.0     7.0   \n",
      "3   4        17.0     1.0       1.0       6.0     7.0      6.0  7.0     7.0   \n",
      "4   4        18.0     1.0       1.0       6.0     7.0      6.0  7.0     7.0   \n",
      "\n",
      "   gaming  ...  gaming_2  clubbing_2  reading_2  tv_2  theater_2  movies_2  \\\n",
      "0     5.0  ...       5.0         4.0        9.0   2.0        4.0       8.0   \n",
      "1     5.0  ...       3.0         5.0        6.0   6.0        4.0       7.0   \n",
      "2     5.0  ...       7.0         7.0        6.0   8.0       10.0       8.0   \n",
      "3     5.0  ...       2.0         6.0        4.0   2.0        7.0       9.0   \n",
      "4     5.0  ...       4.0         2.0        6.0   9.0        3.0       9.0   \n",
      "\n",
      "   concerts_2  music_2  shopping_2  yoga_2  \n",
      "0         7.0      8.0         5.0     1.0  \n",
      "1         7.0      9.0         5.0     5.0  \n",
      "2         9.0      9.0         8.0     1.0  \n",
      "3         7.0      7.0         2.0     2.0  \n",
      "4         3.0      6.0         2.0     1.0  \n",
      "\n",
      "[5 rows x 37 columns]\n",
      "Empty DataFrame\n",
      "Columns: [id, gender, wave, num_dates, partner_id, same_race, partner_age, age, race, sports, tvsports, exercise, dining, museums, art, hiking, gaming, clubbing, reading, tv, theater, movies, concerts, music, shopping, yoga, attr3_1, sinc3_1, fun3_1, intel3_1, amb3_1, decision, attr, sinc, intel, fun, amb, prob, field, career, age_diff]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 41 columns]\n",
      "(4254, 42)\n"
     ]
    }
   ],
   "source": [
    "# interest_diff\n",
    "interest_df = similarity_df[['id', 'partner_id', 'sports', 'tvsports', 'exercise', 'dining',\n",
    "       'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv',\n",
    "       'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga']]\n",
    "interest_df_partner = interest_df.drop([\"partner_id\"], axis = 1).groupby(\"id\").mean()\n",
    "interest_df_partner = interest_df_partner.reset_index()\n",
    "interest_merged = duckdb.sql(\"SELECT * FROM interest_df a LEFT JOIN interest_df_partner b ON a.partner_id = b.id\").df()\n",
    "print(interest_merged.head())\n",
    "\n",
    "interest_merged[\"interest_diff\"] = 0\n",
    "for self_interest in ['sports', 'tvsports', 'exercise', 'dining',\n",
    "       'museums', 'art', 'hiking', 'gaming', 'clubbing', 'reading', 'tv',\n",
    "       'theater', 'movies', 'concerts', 'music', 'shopping', 'yoga']:\n",
    "    partner_interest = self_interest + \"_2\"\n",
    "    interest_merged[\"interest_diff\"] += interest_merged[partner_interest] - interest_merged[self_interest]\n",
    "\n",
    "print(similarity_df[similarity_df[\"id\"] == 416])\n",
    "interest_merged\n",
    "similarity_df = pd.concat([similarity_df, interest_merged[[\"interest_diff\"]]], axis = 1)\n",
    "similarity_df = similarity_df.dropna()\n",
    "print(similarity_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "2db2284a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  partner_id  attr3_1  sinc3_1  fun3_1  intel3_1  amb3_1  id_2  \\\n",
      "0  4.0        11.0      7.0      8.0     9.0       7.0     8.0  11.0   \n",
      "1  4.0        12.0      7.0      8.0     9.0       7.0     8.0  12.0   \n",
      "2  4.0        13.0      7.0      8.0     9.0       7.0     8.0  13.0   \n",
      "3  4.0        17.0      7.0      8.0     9.0       7.0     8.0  17.0   \n",
      "4  4.0        18.0      7.0      8.0     9.0       7.0     8.0  18.0   \n",
      "\n",
      "   attr3_1_2  sinc3_1_2  fun3_1_2  intel3_1_2  amb3_1_2  \n",
      "0        8.0        9.0       7.0         8.0       5.0  \n",
      "1        9.0        9.0       9.0        10.0       9.0  \n",
      "2        4.0        7.0       8.0         8.0       3.0  \n",
      "3        7.0        7.0       6.0         8.0       4.0  \n",
      "4        6.0        8.0       6.0         8.0       9.0  \n",
      "         id  partner_id  attr3_1  sinc3_1  fun3_1  intel3_1  amb3_1   id_2  \\\n",
      "0       4.0        11.0      7.0      8.0     9.0       7.0     8.0   11.0   \n",
      "1       4.0        12.0      7.0      8.0     9.0       7.0     8.0   12.0   \n",
      "2       4.0        13.0      7.0      8.0     9.0       7.0     8.0   13.0   \n",
      "3       4.0        17.0      7.0      8.0     9.0       7.0     8.0   17.0   \n",
      "4       4.0        18.0      7.0      8.0     9.0       7.0     8.0   18.0   \n",
      "...     ...         ...      ...      ...     ...       ...     ...    ...   \n",
      "4249  361.0       356.0      7.0      7.0     5.0       7.0     9.0  356.0   \n",
      "4250  363.0       347.0      5.0      7.0     8.0       8.0     3.0  347.0   \n",
      "4251  363.0       349.0      5.0      7.0     8.0       8.0     3.0  349.0   \n",
      "4252  364.0       347.0      8.0      8.0     8.0       9.0     9.0  347.0   \n",
      "4253  364.0       349.0      8.0      8.0     8.0       9.0     9.0  349.0   \n",
      "\n",
      "      attr3_1_2  sinc3_1_2  fun3_1_2  intel3_1_2  amb3_1_2  char_diff  \n",
      "0           8.0        9.0       7.0         8.0       5.0       -2.0  \n",
      "1           9.0        9.0       9.0        10.0       9.0        7.0  \n",
      "2           4.0        7.0       8.0         8.0       3.0       -9.0  \n",
      "3           7.0        7.0       6.0         8.0       4.0       -7.0  \n",
      "4           6.0        8.0       6.0         8.0       9.0       -2.0  \n",
      "...         ...        ...       ...         ...       ...        ...  \n",
      "4249        7.0       10.0       8.0         9.0       6.0        5.0  \n",
      "4250        8.0        6.0      10.0        10.0      10.0       13.0  \n",
      "4251        7.0        8.0       8.0         9.0       7.0        8.0  \n",
      "4252        8.0        6.0      10.0        10.0      10.0        2.0  \n",
      "4253        7.0        8.0       8.0         9.0       7.0       -3.0  \n",
      "\n",
      "[4254 rows x 14 columns]\n",
      "(3370, 43)\n"
     ]
    }
   ],
   "source": [
    "# char_diff\n",
    "interest_df = similarity_df[['id', 'partner_id', 'attr3_1','sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1']]\n",
    "interest_df_partner = interest_df.drop([\"partner_id\"], axis = 1).groupby(\"id\").mean()\n",
    "interest_df_partner = interest_df_partner.reset_index()\n",
    "interest_merged = duckdb.sql(\"SELECT * FROM interest_df a LEFT JOIN interest_df_partner b ON a.partner_id = b.id\").df()\n",
    "print(interest_merged.head())\n",
    "\n",
    "interest_merged[\"char_diff\"] = 0\n",
    "for self_interest in ['attr3_1','sinc3_1', 'fun3_1', 'intel3_1', 'amb3_1']:\n",
    "    partner_interest = self_interest + \"_2\"\n",
    "    interest_merged[\"char_diff\"] += interest_merged[partner_interest] - interest_merged[self_interest]\n",
    "\n",
    "interest_merged\n",
    "similarity_df = pd.concat([similarity_df, interest_merged[[\"char_diff\"]]], axis = 1)\n",
    "similarity_df = similarity_df.dropna()\n",
    "print(interest_merged)\n",
    "print(similarity_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d677617b",
   "metadata": {},
   "source": [
    "## Data Limitations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49819de6",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "id": "c11f3cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5493, 41)"
      ]
     },
     "execution_count": 835,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# R: general: dimension\n",
    "dating_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "0260d04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: one-variable: mean, sd, #observations\n",
    "# for numerical variable: dating_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "0ed91864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R: one-variable: plot distribution\n",
    "# histogram for numeric variable (reference: kaggle example)\n",
    "# pie chart for categorical variable (reference: discussion example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "24551206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F: two variable\n",
    "# change over Time1, during event, Time 2, Time 3\n",
    "# correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
